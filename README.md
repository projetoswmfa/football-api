# ğŸˆ Sports Data API

Uma API completa para scraping e anÃ¡lise de dados esportivos com inteligÃªncia artificial usando Gemini.

## ğŸš€ Funcionalidades

### âœ… Scrapers Implementados
- **Sofascore**: Partidas ao vivo, placares, estatÃ­sticas em tempo real
- **Transfermarkt**: Elencos, jogadores, valores de mercado
- **Sistema de Rate Limiting**: ProteÃ§Ã£o contra bloqueios

### ğŸ¤– AnÃ¡lises com IA (Gemini)
- **PrevisÃµes de Partidas**: AnÃ¡lise preditiva baseada em dados histÃ³ricos
- **TendÃªncias de Apostas**: IdentificaÃ§Ã£o de oportunidades de valor
- **Forma dos Times**: AnÃ¡lise da performance recente
- **Performance de Jogadores**: AvaliaÃ§Ã£o individual detalhada

### ğŸ“Š Banco de Dados (Supabase)
- **Times**: InformaÃ§Ãµes completas, logos, estatÃ­sticas
- **Jogadores**: PosiÃ§Ãµes, idades, valores de mercado
- **Partidas**: Dados completos incluindo ao vivo
- **AnÃ¡lises**: HistÃ³rico de todas as anÃ¡lises da IA

### âš¡ Recursos AvanÃ§ados
- **Agendamento AutomÃ¡tico**: Scrapers executam automaticamente
- **Rate Limiting Inteligente**: Evita bloqueios de IP
- **Cache e Performance**: Otimizado para alta velocidade
- **Monitoramento**: Logs detalhados e mÃ©tricas

## ğŸ› ï¸ Tecnologias

- **Backend**: FastAPI + Python 3.11+
- **Banco de Dados**: Supabase (PostgreSQL)
- **IA**: Google Gemini Pro
- **Scraping**: ScraperFC, Playwright, BeautifulSoup
- **Agendamento**: APScheduler
- **Logging**: Loguru

## ğŸ“‹ PrÃ©-requisitos

- Python 3.11+
- Conta Supabase
- Chave da API do Google Gemini
- Redis (opcional, para cache)

## ğŸ”§ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone <seu-repositorio>
cd sports-data-api
```

### 2. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 3. Configure as variÃ¡veis de ambiente
Crie um arquivo `.env` baseado no `config.py`:

```env
# Database - Supabase
SUPABASE_URL=https://seu-projeto.supabase.co
SUPABASE_KEY=sua-chave-supabase-anon
DATABASE_URL=postgresql://postgres:senha@db.supabase.co:5432/postgres

# Google Gemini AI  
GEMINI_API_KEY=AIzaSyBpFb-rdZIVGIs-oZQ7VJlEnbPJGeTNnzI

# ConfiguraÃ§Ãµes da API
DEBUG=True
PORT=8000
LOG_LEVEL=INFO

# Rate Limiting
REQUESTS_PER_MINUTE=60
MAX_CONCURRENT_REQUESTS=10

# Scrapers
ENABLE_LIVE_SCRAPING=True
ENABLE_TEAM_SCRAPING=True
ENABLE_PLAYER_SCRAPING=True

# Ligas para scraping
LEAGUES=brasileirao,champions-league,premier-league,la-liga
```

### 4. Configure o banco de dados
Execute o schema SQL no seu projeto Supabase:

```bash
# No dashboard do Supabase, execute o conteÃºdo de schema.sql
```

### 5. Instale o Playwright (para scraping avanÃ§ado)
```bash
playwright install
```

## ğŸš€ Executando a API

### Desenvolvimento
```bash
python main.py
```

### ProduÃ§Ã£o
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8000`

## ğŸ“– DocumentaÃ§Ã£o da API

### Endpoints Principais

#### ğŸ  Health Check
```
GET /              # Status bÃ¡sico
GET /health        # Health check detalhado
```

#### ğŸ† Times
```
GET /teams                    # Lista times com paginaÃ§Ã£o
GET /teams/{team_id}          # Busca time especÃ­fico  
POST /teams                   # Cria novo time
GET /teams/{team_id}/players  # Jogadores do time
```

#### ğŸ‘¤ Jogadores
```
GET /players/{player_id}      # Busca jogador especÃ­fico
POST /players                 # Cria novo jogador
```

#### âš½ Partidas
```
GET /matches                  # Lista partidas com filtros
GET /matches/live             # Partidas ao vivo
GET /matches/{match_id}       # Busca partida especÃ­fica
POST /matches                 # Cria nova partida
```

#### ğŸ”„ Scraping
```
POST /scraping/live-matches         # Dispara scraping ao vivo
POST /scraping/teams/{league}       # Scraping de times por liga
```

#### ğŸ¤– AnÃ¡lises IA
```
POST /analysis/match-prediction     # PrevisÃ£o de partida
POST /analysis/betting-trends       # TendÃªncias de apostas
POST /analysis/team-form           # Forma do time
GET /analysis/{analysis_id}        # Busca anÃ¡lise especÃ­fica
```

#### ğŸ“Š EstatÃ­sticas
```
GET /stats/summary            # Resumo geral da API
```

### Exemplos de Uso

#### PrevisÃ£o de Partida
```bash
curl -X POST "http://localhost:8000/analysis/match-prediction" \
  -H "Content-Type: application/json" \
  -d '{
    "match_id": 1,
    "include_team_form": true,
    "include_head_to_head": true,
    "analysis_depth": "detailed"
  }'
```

#### Scraping de Times
```bash
curl -X POST "http://localhost:8000/scraping/teams/brasileirao?max_teams=5"
```

#### Buscar Partidas Ao Vivo
```bash
curl "http://localhost:8000/matches/live"
```

## ğŸ” ConfiguraÃ§Ã£o do Supabase

### 1. Crie um novo projeto no Supabase

### 2. Execute o schema SQL
Use o conteÃºdo do arquivo `schema.sql` no SQL Editor do Supabase.

### 3. Configure RLS (Row Level Security)
```sql
-- Exemplo de polÃ­tica RLS para times
ALTER TABLE teams ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Teams sÃ£o pÃºblicos" ON teams
    FOR SELECT USING (true);
```

### 4. Obtenha as credenciais
- **URL**: Na seÃ§Ã£o Settings > API
- **Chave Anon**: Na seÃ§Ã£o Settings > API

## ğŸ¤– ConfiguraÃ§Ã£o do Gemini

### 1. Obtenha uma chave da API
- Acesse: https://makersuite.google.com/app/apikey
- Crie uma nova chave API

### 2. Configure a chave no ambiente
```env
GEMINI_API_KEY=sua-chave-aqui
```

## ğŸ“Š Monitoramento e Logs

### Logs da AplicaÃ§Ã£o
```bash
# Logs em tempo real
tail -f logs/app.log

# Filtrar por nÃ­vel
grep "ERROR" logs/app.log
```

### Status do Scheduler
```bash
curl "http://localhost:8000/stats/summary"
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Rate Limiting
Configure no `.env`:
```env
REQUESTS_PER_MINUTE=60        # Requests por minuto
MAX_CONCURRENT_REQUESTS=10    # Requests simultÃ¢neos
```

### Intervalos de Scraping
```env
LIVE_MATCHES_INTERVAL=60      # 60 segundos para ao vivo
TEAM_STATS_INTERVAL=86400     # 24 horas para times
PLAYER_STATS_INTERVAL=43200   # 12 horas para jogadores
```

## ğŸš¨ Troubleshooting

### Erro de ConexÃ£o com Supabase
```bash
# Teste a conexÃ£o
python -c "
import asyncio
from database import db_manager
async def test():
    await db_manager.initialize_pool()
    print('ConexÃ£o OK')
asyncio.run(test())
"
```

### Erro no Gemini
```bash
# Teste a API do Gemini
python -c "
import google.generativeai as genai
genai.configure(api_key='sua-chave')
model = genai.GenerativeModel('gemini-pro')
response = model.generate_content('Hello')
print(response.text)
"
```

### Rate Limiting do Transfermarkt
- Reduza `max_teams` nos endpoints de scraping
- Aumente os delays no cÃ³digo
- Use proxies rotativos se necessÃ¡rio

## ğŸ“ˆ Performance

### OtimizaÃ§Ãµes Implementadas
- **Pool de ConexÃµes**: AsyncPG com pool otimizado
- **Ãndices de Banco**: Ãndices estratÃ©gicos para queries frequentes
- **Rate Limiting**: Evita bloqueios e overload
- **Async/Await**: Processamento nÃ£o-bloqueante
- **PaginaÃ§Ã£o**: Responses otimizadas

### MÃ©tricas Esperadas
- **Throughput**: ~1000 requests/min
- **LatÃªncia**: <100ms para queries simples
- **Scraping**: ~50 partidas ao vivo/min

## ğŸ”® Roadmap

### PrÃ³ximas Funcionalidades
- [ ] WebSocket para dados em tempo real
- [ ] Cache Redis para performance
- [ ] Dashboard web interativo
- [ ] NotificaÃ§Ãµes push para eventos importantes
- [ ] IntegraÃ§Ã£o com mais sites de dados esportivos
- [ ] API de prediÃ§Ãµes avanÃ§adas
- [ ] Sistema de alertas personalizados

### Melhorias Planejadas
- [ ] Testes automatizados completos
- [ ] Deploy com Docker
- [ ] CI/CD com GitHub Actions
- [ ] DocumentaÃ§Ã£o interativa
- [ ] MÃ©tricas e monitoramento avanÃ§ado

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para detalhes.

## ğŸ†˜ Suporte

- **Issues**: Use o sistema de issues do GitHub
- **DocumentaÃ§Ã£o**: Consulte este README e os comentÃ¡rios no cÃ³digo
- **Performance**: Monitore os logs para identificar gargalos

## ğŸ¯ Casos de Uso

### Para Apostadores
- AnÃ¡lises preditivas baseadas em dados reais
- IdentificaÃ§Ã£o de apostas com valor
- Monitoramento de odds em tempo real

### Para Analistas Esportivos
- Dados histÃ³ricos abrangentes
- ComparaÃ§Ãµes estatÃ­sticas avanÃ§adas
- Insights gerados por IA

### Para Desenvolvedores
- API RESTful completa e documentada
- Exemplos de scraping Ã©tico e eficiente
- IntegraÃ§Ã£o com IA para anÃ¡lises

---

ğŸ’¡ **Dica**: Para comeÃ§ar rapidamente, execute primeiro o scraping de algumas ligas principais e depois teste as anÃ¡lises de IA!

ğŸš€ **Performance**: A API Ã© otimizada para processar milhares de partidas e anÃ¡lises simultaneamente.

ğŸ”’ **SeguranÃ§a**: Rate limiting e validaÃ§Ã£o rigorosa protegem contra abusos.

ğŸ“± **Escalabilidade**: Arquitetura assÃ­ncrona permite fÃ¡cil escalabilidade horizontal. 